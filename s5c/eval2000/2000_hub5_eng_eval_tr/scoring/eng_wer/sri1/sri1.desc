		March 2000 LVCSR Hub-5 Evaluation

		SITE/SYSTEM NAME:    sri/sri1

		TEST DESIGNATION:    Hub-5E


1) MAIN TRANSCRIPTION SYSTEM DESCRIPTION

SUMMARY: The SRI DECIPHER(TM) speaker-independent continuous speech
recognition system used on the Hub-5E diagnostic test set is based on
continuous-density, genonic hidden Markov models (HMMs) [1]. The system
used a multiple-pass recognition strategy [6], with a vocabulary of 32,606
words, cepstral mean and variance normalization in the front-end, vocal
tract length normalization, non-cross word acoustic models and a bigram
interpolated language model (LM) for lattice generation, cross-word
acoustic models and trigram interpolated LM for lattice-decoding.
Means and variances of all acoustic models are adapted to each
conversation side by way of affine Gaussian transforms [7-11].
Multiple additional knowledge sources are used for hypothesis
evaluation, including a class-basses 4-gram LM [12], a speaking-rate
dependent acoustic model [14], a phone-duration model [15], and an
"Anti-LM" of acoustically confusable incorrect hypotheses,

A summary of the processing stages is given below:

 1. Determine gender of conversation side.
 2. Compute cepstra removing cepstral mean, normalizing variances, and
    applying VTLN warping by conversation side.
 3. Unsupervised adaptation of within-word acoustic models to each speaker,
    using a phone-loop pronunciation model.
 4. Generation of 2000-best hypotheses using a bigram LM and speaker-adapted
    acoustic models from step 3.
 5. Rescoring of N-best hypotheses with a trigram LM.
 6. Adaptation of both within- and cross-word acoustic models to the 1-best
    hypotheses obtained in step 5.
 7. Word lattice generation using the speaker-adapted within-word models and
    the bigram LM.
 8. Expansion of bigram lattices to incorporate trigram LM probabilities.
 9a. 500-best recognition constrained by trigram lattices, using
    speaker-adapted, rate-independent cross-word triphone acoustic models.
 9b. 2000-best recognition constrained by trigram lattices, using
    speaker-adapted, rate-dependent cross-word triphone acoustic models.
 9c. 2000-best recognition from bigram LM, using speaker-adapted, rate-
    independent within-word triphone models (Note: this step should normally
    also use trigram lattices, but did not due to an oversight).
10. Rescoring of the N-best lists with 
     	a. class-based 4-gram LM (applied to the output of step 9a)
	b. an "anti-LM" (9a)
     	c. phone duration model (9a)
	a. word-based 3-gram LM (9b, 9c)
11. Merging of N-best lists and computation of word posterior probabilties
    using modified ROVER algorithm.
12. Time alignment of the resulting hypotheses.
13. Estimation of word confidences using a neural network, based on word
    posterior probabilities from step 11 and miscellaneous other features.
14. Formatting of the output for submission.

A. Front-end processing

Our front-end signal processing involves the following steps:  (a)
gender detection and (b) feature normalization.  The front-end
configuration for gender detection is identical to our 1998 evalutation
system and therefore differs from that used in recognition.  Gender
detection is based on a FFT-based spectral analysis every 10msec, which
are integrated into 18 spectral bands from 100 to 3300 Hz, and are used
to compute 9 cepstral coefficients (C1-C9) plus C0.  The mean of the
C1-C8 cepstral coefficients are removed over the whole conversation
using all the sentences (and using all frames) on this side.  From
these 9 cepstral features (C0-C8), first and second derivatives over
time are computed.  These features are concatenated together to form a
27-dimensional cepstral vector, which is used as the single input
feature of the gender detection system.

We then determine the gender of each side of the conversation.  All the
speech segments on this side of the conversation are used to make the
gender determination.  A two state HMM with 256 mixtures is used to
perform gender selection (1 state for male speech, 1 state for female
speech).  The feature for this classification is the cepstral vector
C1-C8 (after mean removal by conversation side).  The probabilities are
accumulated across all sentences for this conversation side and a
decision is made at the end of the conversation side.

After performing initial gender selection, we recompute the features
using a new front-end for use in our recognition system. The front end
signal processing computes a 256 point FFT spectrum every 10msec.
These are then integrated into 24 spectral bands from 100 to 3760 Hz,
from which we compute 12 cepstral coefficients (C1-C12) plus C0. From
these 13 cepstral features (C0-C12), first and second derivatives over
time are computed. These features are concatenated together to form a
39-dimensional cepstral vector. The features are then processed by
doing mean and variance normalization. The mean of the feature is
removed over the whole conversation using all the sentences (and using
all frames) on this side. In addition, the variance of the features is
normalized to unity along all dimensions.

Using the new front-end, we compute a gender-dependent estimate of the
vocal-tract length (VTL), based on the algorithm reported in [2]. To
compute the VTL, we use a 128-Gaussian GMM trained on a subset of the
training data using mean and variance normalized features. The VTL for
the test data (each conversation side) is then computed by maximizing
the likelihood of the mean and variance normalized test features with
respect to the GMM. To compute the optimum VTL, we search over 7
discrete VTL values in the interval [0.94, 1.06]. Once the VTL is
estimated, we use it to recompute the features, which are now
normalized for VTL, mean and variance.

B. Acoustic Model Adaptation

We use maximum-likelihood transformation-based adaptation [8-10] to
adapt the speaker-independent acoustic models to a conversation-side
specific acoustic models.  This is done in two stages.  In the first
stage (step 3 above) we use a context-independent phone loop with all
48 phones to align the models with the data (using the forward backward
algorithm) [11], while in the second stage (step 6) we constrain the
alignment to use the recognition hypotheses of the first stage adapted
acoustic models (transcription-model adaptation).

Specifically we used a block-diagonal transformation of the HMM mean
vectors as described in Neumeyer et al. [10]. This is a modification of
the technique presented previously in [11] where a full-matrix
transform was used. Our experiments showed the block diagonal approach
to produce more robust estimates than the full-matrix approach.
Separate transformations are estimated for different Gaussian clusters
in order to make better use of the data. We used 3 separate
transformations to adapt the phone-loop models. The Gaussians
corresponding to the silence or pause phone were adapted separately as
in Sankar and Lee [7] with one of the transformations.  In
transcription mode adaptation, we used 7 transforms associated with
different phone classes that were hand-defined based on phonetic
features.  For the second stage of adaptation, we also used variance
scaling transformation described by Sankar and Lee [7].

C. Search

Decoding from full bigram models (steps 4, 7 and 9c) used a
time-synchronous forward-backward search [4], where the forward search
was based on a bigram network with lexical tree-structured backoff node
[5].  The goal of recognition step 4 (with trigram rescoring of N-best
output) was to generated high quality hypotheses for transcription-mode
adaptation, while the purpose of step 7 is to generate word lattices
for progressive search with more elaborate acoustic and language
models.

Word lattices are reduced using an iterative algorithm and then
expanded to incorporate trigram LM weights, using the LM's backoff
structure to minimize lattice size [13].  Recognition from lattices
ignores prior acoustic scores and time alignments and uses the word
lattices as a constrained language model.

N-best output from several recognition systems (using different
acoustic and/or language models) were combined using a modified version
of the NIST ROVER algorithm [17].  Our N-best ROVER algorithm 
combines ROVER's weighted voting among different systems with voting
among different N-best hypotheses for the purpose of explicit word
error minimization [18,19].  For each system's N-best list we compute
posterior hypothesis probabilities, and multiply them by a global
weight reflecting the system's reliabilty.  All N-best hypotheses from
all systems are then merged into a single alignment matrix.  For each
position in the alignment the word (or empty hypothesis) with the
highest combined posterior is selected.  The system weights were
optimized by trial-and-error on the 1998 Hub-5 evaluation data.

D. Confidence measures

To compute the confidence in each hypothesized word, we used a neural
network that takes several word-level features as input [20].  The
main input feature used is the word posterior probability obtained from
the N-best ROVER algorithm described above.  Additional minor features
include the overall length of the hypothesis and the normalized
relative position of the word in the word string.  Gender was not
conditioned on.

The neural network was trained on a portion of the 1998 evaluation
data.  System output had been labeled as correct or incorrect by 
a dynamic alignment between the hyp and the ref word strings.  Two
output nodes were used (2 classes: correct and incorrect), with softmax
output layers. The training criterion was cross-entropy minimization.
The network had one hidden layer with 4 hidden nodes.

2) ACOUSTIC TRAINING 

The acoustic training material consisted of (A) Macrophone telephone
speech, (B) 3094 conversation side from the BBN segmented Switchboard-1
training set, with part of the segmentations and transcription replaced
at SRI with hand corrections, and (C) 100 Callhome English training
conversations.  The Callhome training data consisted of 5K male
sentences and 22K female sentences.  The total number of sentences for
training was 121K male sentences and 149K female sentences.
The training data was processed with cepstral normalizations and VTL
normalizations to be consistent with testing.

The acoustic models were trained in two passes. The first set of
acoustic models were trained and used to dump alignments for the
training data.  These alignments were used to seed the second iteration
of acoustic models.  To seed the acoustic models, we use a 12K sentence
subset of the training corpus per gender that consisted of the
hand-checked Switchboard data combined with the callhome-English
training data.  The acoustic models are trained as follows: after
training phonetically tied mixture models, the models were clustered
and genonic acoustic models were trained (4 iterations forward-backward
for phonetically-tied mixtures, 4 iterations for genonic models).  In
the last training iteration we also collected statistics for the choice
of alternate pronuncations.  Phonetic models had standard three-state 
HMM structure with left-to-right transitions and self-loops (enforcing
a minumum duration of 3 frames).

As separate recognition system used speaking-rate-dependent acoustic
models, using the framework described in [14]. Briefly, the system has
"slow" and "fast" models for each context-dependent phone. Training
instances of a word are assigned to either slow or fast models based on
the actual length of a word relative to a cumulative duration
distribution.  In decoding the system can choose either all "slow" or
all "fast" phones for a given word, represented as alternate
pronunciations and associated with prior probabilities.

Acoustic training resulted in 2063 male genones and 2348 female genones
of 64 Gaussians each for the within-word models.  The rate-independent
cross-words models used 3064 male genones and 2721 female genones.  The
rate-dependent cross-word models had 3323 male genones and 2983 female
genones.

Another new knowledge source in this year's system were word duration
models.  The models represent permissible duration patterns of words.
For example, a word with 3 phones will be represented by a
3-dimensional feature vector comprising the durations of the 3 phones.
This representation allows automatic training of models from the
acoustic training data. More details of the approach can be found in
[15,16].

To train the duration models, we generated the phone backtraces of all
utterances in the conversational speech acoustic training data
(excluding Macrophone).  From these, we extracted the duration patterns
of the words, which are then used to train Gaussian Mixture Models
(GMMs) of the duration patterns. To handle words unseen in the training
data, we also trained individual triphone and monophone duration
models.  Whenever an unseen word is encountered first the triphone
models are consulted, and if not found, the context-independent
monophone models are used.  We trained models for all words in training
data which occurred at least 20 times. The number of mixtures in the
GMMs is limited to a maximum of 40.

Duration scoring of the N-best hypotheses proceeds as follows. First we
generate the phone backtraces for all hypotheses in the N-best list. Then
the duration scores for each hypothesis is computed as the sum of the
duration log scores of individual words. The word score is obtained by
computing the likelihood of the word duration pattern given the
duration model for that word. 


3) GRAMMAR TRAINING

The first-pass recognizer used a backoff bigram model containing 34,610
unigrams and 1.3M bigrams (the unigrams correspond to the vocabulary
words plus multiwords and abbreviations; see lexicon description).  The
lattice expansion and N-best rescoring stages used an unpruned trigram
backoff model with 4.8M bigrams and 11.5M trigrams.

N-gram models were built as follows: separate LMs were generated from
the Switchboard data, the CallHome training corpus and the 1996
Broadcast News corpus.  The training data amounts were roughly as
follows:
							#words
	CallHome English, 100 training conversation     210K
	Switchboard I, all 2281 conversations           2.5M
	DARPA 1996 Hub4 Broadcast News training data    130M

CallHome English and Switchboard LMS included all bigrams, and all
trigrams occurring at least twice in the respective corpus.  The Hub4
LM included has a minimum count of 2 for both bigrams and trigrams.
Multiwords and abbreviations were treated as single words and
introduced by textual substitution in the training data.

The three backoff LMs were then combined into a single new backoff
model.  The interpolation weights for the three LMs where 0.4, 0.4 and
0.2, respectively, and were obtained by roughly minimizing perplexity
on the 1996 CallHome evaluation data.  For bigram recognition only, the
resulting interpolated model was then pruned, first by using an entropy
criterion that removed all bigrams that caused less than 10^-8 relative
change in training set perplexity [3].  Second, bigrams that had lower
probability than the corresponding backoff estimate were removed to
avoid search errors in the weighted network encoding the LM in the
recognizer.

In addition, we constructed a 4-gram LM based on automatically induced
word classes, from the Switchboard and CallHome training data alone.
The word classes were computed using the incremental greedy merging
algorithm guided by mutual information described in [12].  The
resulting model was then interpolated (weight 0.25) into the standard
word-based trigram described above, and used for N-best list
rescoring.

A final knowledge source was the "Anti-LM", a trigram model
characterizing hypotheses that were acoustically confusable with
correct word hypotheses.  The model was trained on all N-grams found by
500-best decoding the acoustic training material; N-gram counts were
weighted by the posterior probabilities according to the 500-best
list. Log probabilities from this model were added to the scores from the
class-based LM, with an empirically optimized negative weight, thus
penalizing likely confusable, incorrect N-grams.

The weights for log-linear interpolation of the various LM and 
acoustic scores were chosen to minimize the error rate on the
1998 Hub-5 evaluation data.


4) RECOGNITION LEXICON DESCRIPTION

The recognizer vocabulary included all words found in the CallHome
English training corpus, as well as all words in the Switchboard-I
corpus.  In addition, we added the 10,000 most common words in the Hub4
broadcast news corpus, and took the union of the different
vocabularies.  This resulted in a total vocabulary of 32,606 words.  We
then added 265 frequent multi-letter sequences ("C_N_N") and 1389 word
bigrams and trigrams ("A_LOT_OF") as "multiwords" to the recognizer
vocabulary to improve modeling of cross-word reductions (and also to
extend the average scope of the bigram LM).

The recognition lexicon was based on the CMU V0.4 lexicon.  Stress
information was stripped from the lexicon, but "AH0" was rewritten as
schwa (AX), and flaps (DX) replaced D and T in the appropriate phonetic
contexts.  Two additional phones were dedicated to modeling of filled
pauses (PUH as the vowel in "uh" and "um" and PUM for the nasal in "um").

Multiwords had alternate pronunciations including both the baseforms
and any idiosynchratic or reduced forms, the latter ones created by
hand.  All alternate pronunciations were weighted relative to each
other according to their smoothed frequency of occurance in the
training data, and pronunciations with probability less than 0.3 times
that of the most frequent form were pruned from the lexicon.

In addition to the lexical words, separate models were created for
non-lexical and non-speech phenomena, each with a dedicated phone:

	Label	 Phone	
	@reject@ rej	unintelligible, fragmented and OOV words ("rejects")
	[NOISE]	 bgn	background noise
	[LAUGH]  lau	laughter
	[MOUTH]  mtn	mouth noises (breath, cough, lipsmacks, etc.)
	-pau-    -	inter-word pauses

The resulting phoneset had 48 phones.


5) DIFFERENCES FOR EACH CONTRASTIVE TEST

Relative to SRI's 1998 Hub-5 evaluation system, the current system has the
following differing features:

- added VTL normalization on training data
- added cepstral variance normalization
- added multi-word modeling and other refinements to the dictionary,
  including estimation of pronunciation probabilities
- added variance scaling in adaptation
- larger number of transforms in transcription-mode adaptation
- added trigram lattice decoding
- added cross-word acoustic models
- added rate-dependent acoustic models
- added phone duration models
- added class 4-gram LM rescoring
- added "anti-LM" rescoring
- new N-best ROVER approach for system combination and word posterior 
  probability generation.
- new feature set for neural network confidence estimator


6) NEW CONDITIONS FOR THIS EVALUATION

Not applicable.


7) EXECUTION TIME

The following are approximate multiples of real time on a 400MHz Pentium
system with 512MB of RAM.  (Note: The actual runs for most processing stages
were split among a large number of machines of different speeds, but we
have averaged the relative runtimes while roughly adjusting for differing clock
rates.)

Step	What					xRT

 1	Gender detection			0.1
 2	Feature normalization			0.6
 3	Phone-loop adaptation			18.6
 4	N-best recognition			52.3
 5	N-best rescoring			0.3
 6	Transcription-mode adaptation
	a. within-word models			6.8
	b. cross-word models			33.9
	c. rate-dependent models		31.3
 7	Lattice generation			57.9
 8	Lattice expansion			16
 9	a. N-best from trigram lattices		14.2
 	b. N-best from trigram lattices		27.1
	c. N-best from bigram			47.4
10 	N-best recoring				
	a. word LM				1.2
	b. class LM 				3.7
	c. "anti LM"				1.1
	d. duration model			6.6
11	N-best ROVER				0.4
12	Time alignment				0.6
13	Confidence estimation			< 0.1

	Total					321
	

8) REFERENCES

[1] V. Digalakis and H. Murveit, GENONES: Optimizing the degree of
mixture tying in a large vocabulary hidden Markov model based speech
recognizer, Proc. IEEE ICASSP, pp. I-537 to I-540, 1994.

[2] S. Wegmann, D. McAllaster, J. Orloff, and B. Peskin, Speaker
Normalization on Conversational Telephone Speech, Proc. IEEE ICASSP
Vol.1, pp.339-341, 1996.

[3] A. Stolcke, Entropy-based Pruning of Backoff Language Models.
Proc.  DARPA Broadcast News Transcription and Understanding Workshop,
pp.  270-274, Lansdowne, VA, 1998.

[4] L. Nguyen, R. Schwartz, F. Kubala, and P. Placeway, Search
Algorithms for Software-Only Real-Time Recognition with Very Large
Vocabularies, Proc. ARPA HLT Workshop, 1993.

[5] H. Murveit, P. Monaco, V. Digalakis, and J. Butzberger, Techniques
to Achieve an Accurate Real-Time Large-Vocabulary Speech Recognition
System, Proc. ARPA HLT Workshop, 1994.

[6] H. Murveit, J. Butzberger, V. Digalakis and M. Weintraub,
``Large-Vocabulary Dictation Using SRI's DECIPHER(TM) Speech Recognition
System: Progressive-Search Techniques,'' in Proceedings of the IEEE
International Conference on Acoustics, Speech and Signal Processing,
Minneapolis, Minnesota, April 1993, pp. II-319 - II-322.

[7] A. Sankar and C.-H. Lee, A Maximum-Likelihood Approach to
Stochastic Matching for Robust Speech Recognition, IEEE Transactions on
Speech and Audio Processing, Vol. 4, No. 3, pp. 190--202, 1996

[8] V. Digalakis, D. Rtischev and L. Neumeyer, Speaker Adaptation Using
Constrained Reestimation of Gaussian Mixtures, IEEE Transactions on
Speech and Audio Processing, Vol. 3, No. 5, pp. 357--366, 1995

[9] L. Neumeyer, A. Sankar and V. Digalakis, A Comparative Study of
Speaker Adaptation Techniques, EUROSPEECH, 1995.

[10] C. J. Legetter and P. C. Woodland, Flexible Speaker Adaptation
Using Maximum Likelihood Linear Regression, Proc. SLS Workshop, 1995.

[11] A. Sankar and L. Neumeyer and M. Weintraub, An Experimental Study
of Acoustic Adaptation Algorithms, IEEE ICASSP '96, pp. 713-716.

[12] P. F. Brown, V. J. Della Pietra, P. V. de Souza, J. C. Lai, and
 R. L. Mercer, Class-Based n-gram Models of Natural Language,
Computational Linguistics 18(4), 467-479, 1992.

[13] F. Weng, A. Stolcke, and A. Sankar, Efficient Lattice Representation
and Generation, Proc. ICSLP, vol. 6, 2531-2534, Sydney, 1998.

[14] J. Zheng, H. Franco, F. Weng, A. Sankar, and H. Bratt, Word-level
rate of speech modeling using rate-specific phones and pronunciations,
Proc. IEEE ICASSP, Istanbul, 2000.

[15] R. Rao Gadde, E. Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur,
Prosody Modeling for Speech Recognition and Understanding, Proc.
Hub-5 Conversational Speech Understanding Workshop, Baltimore, 1999.

[16] R. Rao Gadde, Modeling Word Duration for Better Speech Recognition,
submitted to the Speech Transcription Workshop, Univ. of Maryland, College
Park, May 2000.

[17] J.G. Fiscus, A Post-Processing  System  to  Yield  Reduced Word
Error Rates: Recognizer Output Voting Error Reduction (ROVER),  Proc.
IEEE  Automatic  Speech  Recognition  and Understanding  Workshop,
Santa Barbara, CA, 347-352, 1997.

[18] A. Stolcke, Y. Konig, and M. Weintraub, Explicit Word Error
Minimization, Proc. EUROSPEECH 1997.

[19] L. Mangu, E. Brill, and A. Stolcke, Searching for Consensus to
Improve Recognition Output. Hub-5 Conversational Speech Recognition
Workshop, Linthicum Heights, MD, 1998.

[20] M. Weintraub, F. Beaufays, Z. Rivlin, Y. Konig, and A. Stolcke,
Neural-Network Based Measures of Confidence for Word Recognition,
Proc. IEEE ICASSP, vol. 2, 887-890, 1997.

