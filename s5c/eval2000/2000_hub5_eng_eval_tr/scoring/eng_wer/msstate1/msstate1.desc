1. Primary Test System Description: 
-----------------------------------

The MSU system for the March 2000 Hub-5 eval is a cross-word context-dependent
HMM-based system using continuous density mixture Gaussian statistical models
and phonetically tied states. An interpolated bigram language model courtesy
of SRI and derived from the SWITCHBOARD-I corpus, Call Home English, and the
HUB-4 training data is used to generate word lattices. An accompanying
interpolated trigram language model from SRI is used to rescore these
lattices, generating the final hypotheses. All experiments were carried out
with the ISIP public-domain ASR system freely available at 
http://www.isip.msstate.edu/projects/speech/software

The salient features of our evaluation system include:
 1) 12 cepstral acoustic features + log energy are generated as follows:
    * a standard single-tap pre-emphasis filter is applied to the input signal.
    * data is analyzed using a 10 msec frame duration and 25 msec analysis
      window.
    * the log energy is calculated from this analysis window.
    * a Hamming window is then applied to smooth frame boundary effects.
    * the cepstra are derived from an FFT analysis and using a standard
      mel-scale filterbank [1].
    * first and second derivative coefficients of the 12 cepstral + 1 energy 
      feature are appended to yield a 39 dimensional feature vector.

 2) The cepstral features for a segment are normalized using a mean
    subtraction where the mean is calculated from all analysis frames
    belonging to the same conversation side as the segment.

 3) 32 mixture context-independent monophone models were trained from 60 hours
    of SWB-I data. These context independent models were used to generate
    phone alignments of all training data for training of the context-dependent
    triphone models.

 4) Maximum likelihood phonetic decision tree based state-tying was
    employed. The decision tree uses phonetic rules that are based on
    left and right contexts and a tree is grown for each state of each
    base phone (each base phone in our system is a three-state left-to-right
    HMM) Models corresponding to all unseen contexts are generated using
    these decision trees [2].
     
 5) State-tied word-internal and cross-word triphone models where each state
    in the model is a mixture of four Gaussians with diagonal covariances 
    were trained using the SWB-I only. Mixture generation was continued up
    to 16 mixtures with 20 hours of Call Home data added to the training set.

 6) Mixtures were obtained using an iterative splitting and training
    scheme [3]. The progression was from 1 mixture split to 2, then to 4,
    then 8, then 12 and finally 16.

 7) There are a total of 4193 states in the word-internal model set
    and 10618 states in the cross-word model set.
		       
 8) Recognition is performed using the ISIP time-synchronous Viterbi
    decoder [4, 5]. Decoding was performed in two stages.

    * Stage 1 uses the 16-mixture word-internal triphone models and the
      bigram backoff language model to generate word lattices. 

    * Stage 2 rescores the lattices generated in stage 1 using the 16-mixture
      cross-word triphone models and the trigram backoff language model.
      The output of stage 2 is the final hypothesis string of the system.

2. Acoustic Training:
---------------------

 * initial context independent model training as well as context-dependent
   model training up to 4 mixtures used the SWB-I training set defined for
   the 1997 JHU Summer Workshop. This amounts to 60 hours from 2998
   conversations.

 * to build the final 16 mixture context-dependent models we combined the
   60 hours of SWB-I and 20 hours of English Call Home data from 120
   conversations distributed by the LDC.

3. Grammar Training:
--------------------

 * Bigram and trigram backoff language models as well as support for
   both were generously provided by Andreas Stolcke at SRI. It was trained
   from Switchboard, Callhome and BN. The bigram version was used for word
   lattice generation and the trigram for recognition from lattices.

 * The full trigram LM has 3246315 bigrams and 9966270 trigrams.
   The bigram is pruned to exclude bigrams with would be superseded
   by their backoff paths. It contains only 2946321 bigrams. This is then
   pruned further using SRI's entropy-based method [6]  to eliminate
   negligible bigram parameters.

 * The resulting trigram backoff language model received from SRI contained
   138k trigrams, 320k bigrams, and 33k unigrams. The bigram language model
   simply excluded the 138k trigrams.

4. Recognition Lexicon Description:
-----------------------------------
 * The 33,200 entry lexicon was derived from the 22k WS'97 test lexicon [7] and
   was augmented to cover words contained in the SRI language model but not in
   the 22k lexicon. A minimal number of multiple pronunciation words existed.

5. Differences for each Contrastive Test:
-----------------------------------------
 * No contrasts were run

6. New Conditions for This Evaluation:
--------------------------------------
 * This is the first evaluation using the ISIP ASR system.

7. Execution Time: 
------------------
	The processors used for the evaluation were primarily Intel
	Pentium-III 600Mhz machines running Solaris 2.7 and had 1G RAM
	and 9G hard disk. Stage 1 jobs used a maxmimum of ~500M and
	Stage 2 jobs used a maximum of ~300M at runtime.
	
	Stage 1: 1083 hours	(300 xRT)
	Stage 2: 23 hours	(76 xRT)


8. References:
--------------
 1) J. Picone, "Signal Modeling Techniques in Speech Recognition," IEEE
    Proceedings, vol. 81, no. 9, pp. 1215-1247, September 1993. 

 2) J. Odell, "The Use of Context in Large Vocabulary Speech Recognition,"
    Ph.D. Thesis, University of Cambridge, March 1995.

 3) P. Woodland, et. al., "HTK Version 1.5: User, Reference and Programmer
    Manuals, Cambridge University Engineering Dept. and Entropic Research
    Laboratories Inc., 1995.

 4) N. Deshmukh, A. Ganapathiraju and J. Picone, "Hierarchical Search for
    Large Vocabulary Conversational Speech Recognition," IEEE Signal
    Processing Magazine, vol. 1, no. 5, pp. 84-107, September 1999. 

 5) A. Ganapathiraju, et. al., "The ISIP Automatic Speech Recognition Toolkit",
    Institute for Signal and Information Processing, Mississippi State
    University, 1999.

 6) A. Stolcke, "Entropy-based pruning of backoff language models," in
    Proceedings DARPA Broadcast News Transcription and Understanding Workshop,
    (Lansdowne, VA), pp. 270-274, February 1998.

 7) M. Ostendorf, et. al., "Modeling Systematic Variations in Pronunciations
    via a Language-Dependent Hidden Speaking Mode," 1996 LVCSR Summer Research
    Workshop, Research Notes 24, CLSP, Johns Hopkins University, April 1997.


