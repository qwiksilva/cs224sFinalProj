SITE/SYSTEM NAME: 			att/primary

TEST DESIGNATION:			english


------------------------------------------------------------------------

                    Editorial Note by Jonathan Fiscus 

The 'att1-bug' results linked to this system description are
inapropriately higher due to an error in processing. The 'att1-debugged'
results should be considered to accurately reflect the performance of
the system described below.

------------------------------------------------------------------------

1) PRIMARY TEST SYSTEM DESCRIPTION:

The AT&T system's acoustic, lexical and grammatical modelling are described
in subsequent sections.

The recognition proceeded in multiple passes. The first-pass produces
a set of lattices using a baseline gender dependent triphonic acoustic
model and a baseline trigram language model. Both models are run
in parallel, and the one with higher total likelihood determines
the gender of the whole conversation side. The pruning thresholds were
selected so that the lattice WER on was in the 10-15% range for the Switchboard
portion of the Eval '97 data. This recognizer's architecture is described
in [4].

A second step consists of maximum likelihood vocal tract normalization (VTN)
factor estimation and rescoring with the warped cepstra.

A third step consists of maximum likelihood linear regression
adaptation using 7 transformations, one reserved for the silence
and noise gaussian mixtures, and rescoring. This step also
generates another set of output lattices for further rescoring.

A fourth step is identical to the second step, except it is using
four aditional pentaphone models. The models vary along two dimensions:
gender dependent or gender independent and data was variance normalized or
the data was not variance normalized. There is a total of one triphone
and four pentaphone models

A fifth step is identical to the third step, except it is using
all five models. For the pentaphone models, this step is done
by first adapting with one transformation, then using the output
to adapt using seven transformations.

Each of the preceeding steps used trigram language model and
output a word lattice. The language model contribution was then
removed from the lattice and a larger 6-gram language model was
then intersected with the lattice to provide a more accurate
recognition output.

A sixth step (ROVER) combines the results of the fifth step
by intersecting lattices rescored using each system and finding
their finite-state weighted intersection, followed by their best
path using the AT&T finite-state tools [2].


2) ACOUSTIC TRAINING:

All the acoustic models were trained separately on 160 hours of
Switchboard-1 and Call-Home-English data using the pronlex lexicon.
The acoustic modelling consists of a
three-state Hidden Markov model per context-dependent phone, with each
state modelled by one of approximately 90000-13000 continuous,
variable density distributions [2].  Each distribution consists of up to
32 gaussian mixture components of a 39 parameter feature vector of 12
mel-based cepstral parameters, log energy and their first and second
derivatives.The total number of mixture components in the models is
approximately 120,000 and 150,000.  The feature vectors are normalized
with conversation-wide cepstral mean subtraction.  Adaptation of
acoustic models was done using all of the available speech for each
conversation side using linear regression. 
Next we use seven clusters of state dependent observation mean
values and adapt them using the segmentation of the test utterances
provided by the speech recognition system.

All the acoustic models use 7-class covariance tying based on
broad phonetic features (front-back vowels, voiced-unvoiced fricatives,
stops, nasals and glides). These same classes are used as classes for 
adaptation.

Three types of models were built, baseline for the first pass, model
based on the data that underwent VTN transformations and SAT trained
models also using the transformed training data.

3) GRAMMAR TRAINING:

The language model consists of a trigram and 6-gram
(Katz) backoff model trained on three million words of Switchboard,
200 thousand words of Call Home English text and 120 million words
from the Broadcast News corpus.
In addition to the above data we also used the following which we
obtained from the listed internet locations:

Soap Operas                     4.5 Million words (Closed Captioning)
  Combination of 
   "All My Children"    http://members.tripod.com/~LauraAMC
   "Another World"      http://members.tripod.com/~Laura_AW
   "General Hospital"   http://members.tripod.com/~GHTranscripts
   "One Life to Live"   http://members.tripod.com/~LauraOLTL
   "Port Charles"       http://members.tripod.com/~PCTranscripts

"Friends"                               141K words (Transcripts)
   http://www.geocities.com/Hollywood/9151

"Sabrina the Teenage Witch"             243K words (Transcripts)
   http://www.bccnet.force9.co.uk/transcripts/

"Real Hollywood Talk"                   1.3 Million words (Transcripts)
   http://www.universalstudios.com/unichat.30/newchat/transcripts


4) RECOGNITION LEXICON DESCRIPTION:

The vocabulary consists of 40,000 word PRONLEX Switchboard lexicon
augmented with multiple pronunciations using the methods developed
by the 1997 pronunciation modelling team at that workshop [1]. 


5) DIFFERENCES FOR EACH CONTRASTIVE TEST:

none

6) NEW CONDITIONS FOR THIS EVALUATION:

none

7) EXECUTION TIME:

 1. First-pass lattice generation (correct gender)			 60.0 x rt
 2. First-pass lattice generation (incorrect gender)			125.0 x rt
 3. Estimation of VTN factors (all 5 models)	 			 10.0 x rt 
 4. Rescoring with warped cepstra (triphone)				  4.5 x rt 
 5. Rescoring with warped cepstra (all pentaphones)		        180.0 x rt 
 6. MLLR (triphone) 							  1.0 x rt
 7. Rescoring with adapted models and lattice generation	 	 57.4 x rt
 8. Rescoring with warped cepstra (triphone)				  4.5 x rt 
 9. Rescoring with warped cepstra (all pentaphones)		        180.0 x rt 
10. MLLR for all models		 					  5.0 x rt
11. Rescoring with adapted models (triphone)			 	  3.2 x rt
12. Rescoring with adapted models (pentaphone, 1 transformation)	108.6 x rt
13. MLLR for all pentaphone models (7 transformations)			  4.0 x rt
14. Rescoring with adapted models (pentaphone, 7 transformation)	 76.2 x rt
15. ROVER combination                             			  0.1 x rt

TOTAL:							 		819.5 x rt

Memory usage was maximum in the VTN pentaphone model rescoring, 
which required upwards of two  gigabyte for the 'worst' utterances.

The processing was done on SGI Origin 195MHZ R10000 machines.

8) REFERENCES:

[1] Byrne, W, et al., Pronunciation modelling using a hand-labelled
corpus for conversational speech recognition. Proc. ICASSP '98
Seattle, WA.

[2] Hindle, D., Ljolje, A. and Riley, M., 
Recent improvements to the AT&T Speech-To-Text (STT) System
Proc. ARPA Speech Recognition Workshop, Feb. 1996. 

[3] Mohri, M, Pereira, F. and Riley, M., "A rational design for a
weighted finite-state transducer library", Lecture Notes in Computer
Science, 1436, 1998.

[4] Mohri, M., Riley, M., Hindle, D. Ljolje, A, and Pereira, F.,
"Full Expansion of Context-Dependent Networks in Large Vocabulary
Speech Recognition". Proc. ICASSP '98. Seattle, WA.
